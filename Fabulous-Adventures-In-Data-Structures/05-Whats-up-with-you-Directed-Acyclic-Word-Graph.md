# 你怎么了，有向无环词图？

 **本章涵盖**

- 用于存储大规模词表并按前缀搜索的多种技术
- 哈希集、排序列表、前缀树（trie）和词图（DAWG）的成本与收益
- 在给定已排序词表的情况下构建 tries 和 DAWGs 的算法
- DAWG 与自动机理论之间的联系

在序言中我提到过那场灾难性的第一次面试，而当天我的状况在第二次面试里只略微有所好转。第二位面试官问了我一个更好的问题：给定 Scrabble（拼字游戏）中你手上那七个字母，应该用什么算法找出所有的 “bingos”？也就是说，如何列出由这些字母恰好组成的所有合法七字母单词？这个函数应当以诸如 “NISATEV” 的字符串作为输入，并生成 “NAIVEST”、 “NATIVES” 和 “VAINEST” 这三个输出。

这不是个特别难的问题，我对自己的解答也挺满意，但随后的更难的词表搜索问题把我难住了。我知道前缀树 —— “trie” —— 是适用于词表的数据结构，但对细节不太清楚。当面试官问我是否知道什么是 “dog” 的时候，我直言不知；结果他指的是 Directed Acyclic Word Graph，也就是 DAWG。这是我那天的第二个 “不录用”。

一个好的面试题应该是务实的；涉及词表且对性能敏感的问题极其常见，现在更是比我 1996 年面试时常见得多。打字自动补全就是一个很好的例子。生成并检查文字游戏中的可能走法也是另一个例子。用什么数据结构来存储一个庞大的词表，使其可以被快速搜索，又不占用太多内存？

我们首先从一种简单直接的方案开始：保留一个排序好的词列表。然后我们再考虑对这些单词的前缀去重的成本与收益，最后也对后缀去重；节省可以非常可观。在此过程中，我们还会看看词表问题与有限状态机之间的联系。

## 两个问题，两种弱解法

两个常见的词表问题是：“给定单词是否在列表中？”以及“列表中所有以某给定前缀开头的单词有哪些？”后者对自动补全尤为重要。自动补全问题要求非常快速地产生候选列表，因为用户打字很快！在我那个年代，微软的 C# IDE 团队有 30 毫秒的预算——大约是刷新屏幕所需的时间——来生成 IntelliSense 的补全列表。

照例，让我们先构思一个抽象数据类型，通过接口的形式来表达我们需要的操作。然后我们再思考哪些数据结构适合作为这个 ADT 的实现：

```csharp
interface IWords : IEnumerable<string>
{
    bool Contains(string word);
    IEnumerable<string> StartsWith(string prefix);
}
```

这里包含了三个特性。第一，词表应当是一个可枚举的字符串序列。第二，我们需要能够判断某个给定单词是否在列表中。最后，我们需要能够枚举所有以某个前缀开头的单词。

> **注意**
> 我这里只讨论如何表示一个已有的词表以便高效搜索。关于如何在内存中动态添加或移除词条并维持某种搜索性能标准的问题，则不在这里讨论。

在我们进入 tries 之前，让我们快速考虑两种可能的词表存储方案，它们都可以实现这些操作。在本章所有的性能分析中，我将使用 n 表示列表中的单词数量，m 表示单词长度；我意指最大长度还是平均长度，将由上下文明确。

### 哈希集并不是解法

我们可以立即排除将词表存储在 `HashSet<string>` 中的做法。为什么？

哈希集在 `Contains` 操作上表现极佳。哈希集的工作方式是首先计算字符串的整数“哈希码”，通常通过对其每个字符按顺序执行一些简单的数学操作实现。然后它会将词表划分到若干“桶”中，使得所有哈希码相似的单词落在同一个桶内。例如，可能会有八个桶，每个桶中所有字符串的哈希码的最低三位相同。通过在 n 增加时巧妙地调整桶的数量，哈希集可以实现令人印象深刻的 O(m) 的 `Contains` 性能。也就是说，哈希集的渐进性能不依赖于所存储列表的大小，而只依赖于哈希函数的性能。由于哈希函数会处理字符串中的每个字符，因此该操作是 O(m)。

不幸的是，哈希集几乎无法用于实现 `StartsWith` 操作。一个设计良好的哈希算法会将不相似的单词放入同一个桶中。（我曾经用一个把相似单词放入相似桶的糟糕哈希算法毁掉过 Microsoft.com 的早期版本的性能，但那是另外一个故事！）没有任何高效的方法可以用哈希集实现这个操作。我们不会再进一步考虑这个选项，不过我们将在本章后面回到一个关于安全使用哈希集的重要规则。

如果不能使用哈希集，那我们能做什么呢？

### 排序列表算是……还能接受？

要让词表在常用操作中获得不错的性能，最简单的存储方式毫不意外，就是用 `List<string>`。只要我们提前把列表排好序 —— 这通常不是什么难事 —— 就能利用二分搜索，让性能变得相当可观。我们直接实现一下；代码很短：

代码清单 5.1 排序词表实现

```csharp
class SortedWordList : IWords
{
    private List<string> words;
    public SortedWordList(List<string> words) #A
    {
        this.words = words;
    }
    public bool Contains(string word) =>
        words.BinarySearch(word) >= 0; #B
    public IEnumerable<string> StartsWith(string prefix)
    {
        int i = words.BinarySearch(prefix);
        if (i < 0)
            i = ~i; #C
        for (; i < words.Count; i++)
        {
            string w = words[i];
            if (!w.StartsWith(prefix)) #D
                yield break;
            yield return w;
        }
    }
    public IEnumerator<string> GetEnumerator() =>
        words.GetEnumerator();
    IEnumerator IEnumerable.GetEnumerator() => GetEnumerator();
}
```

`Contains` 的逻辑很直接：对目标单词做二分查找。如果查不到，`BinarySearch` 会返回负数，因此只要看符号就行了。

`StartsWith` 则稍微曲折一点。如果没有与 `prefix` 完全一致的项，`BinarySearch` 会返回排序表中“下一个更大元素”的按位取反值。找到起点后，我们就一路往后检查，直到遇到不再符合前缀的单词，或到达列表末尾。

> `BinarySearch` 在找不到目标时，**绝不仅仅是告诉你“没找到”（返回 -1）这么简单**，它实际上还附赠了一条非常有用的信息：**“虽然没找到，但我知道它如果存在，应该排在哪里。”**
>
> 设计思路：既然二分查找的过程中已经确定了它应该在的位置，不如直接把这个位置通过返回值告诉你。
>
> #### 例如：
>
> 假设列表是：`list = ["A", "C", "E"]` （注意没有 "B" 和 "D"） 索引分别是：`0, 1, 2`。需要找 "B"
>
> 1. 二分查找在列表中找 "B"。
> 2. 没找到。
> 3. 但是它发现 "B" 比 "A" 大，比 "C" 小。
> 4. 如果要把 "B" 插进去，应该插在 "C" 的位置，也就是索引 **1**。
> 5. 为了表示“没找到”且“应该在1”，它会对 1 进行按位取反。
>    - 在计算机补码表示中，`~1` 等于 `-2`。
> 6. **结果：** `BinarySearch` 返回 **-2**。

分析性能时，可以合理假设参数 `word` 和 `prefix` 长度大约为 m，也就是词表的典型单词长度。二分搜索要做 O(log n) 次比较，而字符串比较本身要 O(m)，因此两个方法的搜索阶段都是 O(m log n)。但 `StartsWith` 除此之外，还要对每一个返回值调用一次 `StartsWith`，这也要 O(m)，而匹配数量不确定 —— 最坏可能是整个列表。

到目前为止，我们多半关注渐进意义上的性能：随着问题规模增大，算法如何扩展，而非其实际耗时和内存。

我们习惯认为现代硬件几乎有无限内存，物理内存不足 OS 会自动用页面文件，GC 也替我们处理垃圾。但我学编程的时代就不同了，PC 的可用内存被 640K 生生卡死，上磁盘意味着换软盘。如果我们真得把词表尽可能压缩，会怎样？到底占多少字节？

本章中我将使用 ENABLE（增强北美基准词典）作为参考词表。这是一份为现代英文文字游戏设计的高质量开源词表。我使用的版本共有 172,820 个单词，字符总数 1,570,508 个。平均长度略超 9，最长的单词有 28 个字母。

> 旁注：
> `ethylenediaminetetraacetates` 虽然工业用途广泛，但在 Scrabble 里根本放不下。

把 ENABLE 放进 `List<string>` 中，内存消耗大约如下：

- C# 字符串使用双字节字符，光字符部分就约 3MB。
- 列表中有 172,820 个字符串引用，每个引用在 64 位机器上 8 字节，共约 1.3MB。
- 每个字符串还有 4 字节长度字段，以及每个 .NET 对象额外 16 字节对象头，加起来又是 3.5MB。

存这些字符串就快 8MB 了。我们当然可以做得更好。例如 ENABLE 的字母只用到 26 个字符，完全可以用 UTF-8 这种一字节编码。光是这一步就能省 1.5MB。

此外，GC 还要处理这 172,820 个字符串。每次回收都要判断这些对象是否存活，非常耗时。

我们可以把所有字符塞进一个巨大的字节数组，用一组 4 字节整数来表示每个字符串的位置和长度，彻底避免几十万对象的引用。这样可能把总占用压到大约 2.5MB。

字符串列表虽然简单易用，空间开销却不算低。我们已经看到：如果真的需要省内存，实现一个自定义 UTF-8 字节数组结构完全可行，只是写起来比较麻烦。本章后面会继续探讨这类技巧。

现在让我们回到渐进性能：对于词表的常见操作，我们能否把时间复杂度做到比 O(m log n) 更低？前缀树可以做到，但它是否意味着要用更多内存换更快的搜索呢？

## 前缀树

当你有大量单词但字母表很小的时候，难免会有很多单词有相同的开头。举例来说，在 ENABLE 中，带有前缀“anti”的单词就有 750 多个。在我们的 List 基准测试里，仅仅是“anti”这几个字符的重复拷贝就占了 6 KB 以上！正是想要去重所有这些公共前缀，才促使我们使用前缀树——也就是后来人们常说的 tries。

**名字从何而来？**

之所以叫 tries，是因为它们用来检索（retrieve）单词，懂了吗？这是个有点冷的玩笑。

我们可以构建一棵树，让每一条边代表一个字符，每一个节点代表单词中的一个分割点，把单词分成前缀和后缀；节点就是字符之间的“空隙”。用一张图会更好地说明这一点。我们先列出九个单词：

```csharp
List words = ["car", "care", "cares", "cars", "fir", "fire", "firer", "firers", "firs"];
```

图 5.1 展示了表示这份单词列表的 trie：

![](asserts/05-01.png)

图 5.1 这个 trie 对应一个包含九个单词的列表，这里从左到右展开，并且从上到下按字母顺序排列。节点编号并不是树中的数据；它们只是为了方便我们在后面的讨论中引用。双圆节点表示单词的结尾。

通常我们画树的时候，会有点反讽地把根画在最上面；我们会认为遍历一棵树是从上到下进行的。由于 trie 表示的是一组单词，在本章的图示中，我会把根放在左边，我们则从左到右遍历这棵 trie。

在我们的单词列表中，前缀“car”出现了四次。在 trie 中，所有带有前缀“car”的单词都由节点 3 表示，因为从根到节点 3 的边拼出了“car”。“car”的所有后缀则由节点 3 右边的所有边表示。

由于“car”本身就是列表中的一个合法单词，我们把节点 3 画成双圆以标记这一事实。节点 2 代表所有以前缀“ca”开头的单词，只画单圆，因为“ca”本身并不是列表中的单词。

接下来我们来看一下如何表示这棵 trie 的节点和边，然后用这份单词列表构建出一棵 trie，之后就可以实现我们的单词列表接口了。

### 节点和边

对于每个节点，我们只需要知道三件事：

- 这个节点是不是某个单词的结尾？
- 从这个节点出发的边上的字符标签是什么？
- 这些边指向哪些节点？

我们只会从左到右遍历这棵树，因此没有必要再保留指向“父”节点的引用。单词结尾的标记可以是一个 `bool` 属性。出边可以是一个`Dictionary<char, Node>`，其中每条边的标签由那个 `char` 表示。

在前几章中，我大力称赞过不可变数据类型的优点，但我并不在这件事上教条。构造一棵 trie 时，是从左到右、从根到叶进行的，而不可变树最适合自底向上（从叶到根）构造，所以使用可变数据结构会更方便。本章稍后在构建 DAWG 时，我们还会对节点做更复杂的修改。所有这些因素促使我把节点设计为可变的。我们先写点代码：

Listing 5.2 A trie node

```csharp
sealed class Node
{
    private readonly Dictionary<char, Node> edges = [];
    public bool IsEnd { get; set; }
    public IEnumerable EdgeLabels => edges.Keys;
    public Node FollowEdge(char c) => edges[c]; #A
    public Node? TryFollowEdge(char c) => #B
    	HasEdge(c) ? FollowEdge(c) : null;
    public bool HasEdge(char c) => edges.ContainsKey(c);
    public Node AddNewNode(char c) => edges[c] = new();
    public Node TryAddNewNode(char c) =>
    	HasEdge(c) ? FollowEdge(c) : AddNewNode(c);
}
```

我本可以把 `edges` 做成 public，但我更倾向于把它设为 private，并提供一些简单的访问方法，理由有两点：

- 最重要的是，这样能让使用这个类的代码更加自描述。这个类型的公共接口应该用“节点”和“边”的专业术语来描述在 trie 节点上可以执行的逻辑操作，而不是用字典的术语。如果我们把字典公开，并要求这个类型的使用者直接与字典打交道，那就是在凸显实现机制，而模糊代码的含义。
- 隐藏实现机制是良好的面向对象设计；我们希望能够自由尝试不同的实现方式，而不用修改所有使用这个类型的代码。我们可能会发现字典的性能特征不能满足我们的需求，然后去寻找别的解决方案。如果那时所有调用方都不得不修改，那就太糟糕了。

我们快速过一遍这个小类的成员：

- `IsEnd` 属性顾名思义：它说明这个节点是否表示列表中某个单词的结尾。
- `EdgeLabels` 列出了从该节点出发的每条边上的标签。
- `FollowEdge` 接收一个边标签，并返回那条边所指向的节点。比如，再回到图 5.1。节点 9 表示所有以前缀“fir”开头的单词，而从它出发的边则表示所有以该边标签开头的后缀。如果我们在它上面调用 FollowEdge('e')，就会得到节点 10，它表示所有以前缀“fire”开头的单词。如果不存在这样的边，这段代码会抛出异常。
- `TryFollowEdge` 与 `FollowEdge` 基本相同，但如果没有这样的边，就返回 null 而不是抛异常。
- `HasEdge` 报告给定的边是否存在。
- `AddNewNode` 创建一个新节点，并强制创建一条到该节点的边。
- `TryAddNewNode` 会检查是否已经存在给定标签的边，如果有就返回那条边所指向的节点；否则，它的行为就和 `AddNewNode` 一样。

在继续构建 trie 之前，我们先简单想一想这些成员的性能特征。这个类型只是一个由单个字符作键的字典的轻封装而已。所有这些操作在时间和空间上都是 O(1) 的。至于这些节点占用了多少字节，我们会在本章稍后再讨论。

### 构建 trie 的算法

从一个单词列表构建一棵 trie 的算法出乎意料地直接：

- 如果要加入的新单词与 trie 中已有的任何单词都没有公共前缀，那么我们只需要从根节点开始，为该单词新建一条由边和节点组成的路径即可。
- 如果要加入的新单词与 trie 中已有的某个单词有公共前缀，那么就找到在该处“切开”已有单词的那个节点，并从那个点开始把新单词剩下的部分接上去；这样就保证了公共前缀被去重。
- 最后，把最后添加的那个节点标记为单词的结尾。

来看一下代码：

Listing 5.3 A trie builder

```csharp
sealed class TrieBuilder
{
    private Node root = new(); #A
    private TrieBuilder() { } #B
    static public Node MakeTrie(IEnumerable words)
    {
        var tb = new TrieBuilder();
        foreach (var word in words)
        	tb.Add(word);
        return tb.root;
    }
    private void Add(string word)
    {
        Node n = root;
        foreach (char c in word)
        	n = n.TryAddNewNode(c);
        n.IsEnd = true;
    }
}
```

我们从一个根节点开始，它没有任何出边，也没有被标记为单词结尾；它表示一个空的单词列表。当我们添加第一个单词（比如“car”）时，每一次调用 `TryAddNewNode` 都会生成一个新节点，从而得到图 5.2 中展示的那棵只含一个单词的 trie：

图 5.2 单词列表 “car” 的 trie

![](asserts/05-02.png)

如果列表中的下一个单词是 “care”，那么在尝试添加 “c”、“a” 和 “r” 时，`TryAddNewNode` 会返回已经存在的节点，而在尝试添加 “e” 时才会创建一个新节点：

图 5.3 单词列表 “car”、“care” 的 trie。

![](asserts/05-03.png)

我就不继续赘述了；我相信你已经能看出，随着更多单词被添加，这棵 trie 是如何一步步演化的。

我们稍后会再来看 trie 的内存性能；现在先看看这个 trie 构建算法的时间性能。我们有一个循环，对列表中所有 n 个单词调用 `Add`，而每次调用 `Add` 又会遍历该单词中所有 m 个字符。总共会有 O(mn) 次对 `TryAddNewNode` 的调用，而我们已经说过，每次调用的时间复杂度是 O(1)。既然在构建 trie 时必须查看单词列表中的每一个字符，那我们也不可能做得比这更好了。

我们最初要解决的动机问题——在单词列表上实现 `Contains` 和 `StartsWith` ——还没有解决；接下来就来看看这一点。

### 把 trie 当作单词列表来用

既然我们已经可以把任意字符串序列变成一棵 trie，那么如何用这棵 trie 来解决最初实现 `Contains` 和 `StartsWith` 的问题呢？我们需要先解决两个更小的问题：

- 给定一个前缀，在 trie 中找到与之关联的节点——如果存在的话。也就是说，如果给定前缀字符串 “car”，就从根开始依次沿着边 ‘c’、‘a’ 和 ‘r’ 往下走。
-  给定一个节点，列出该节点“右侧”的所有单词后缀。在我们之前的那个 9 个单词的 trie 例子中，“car” 的后缀是 “”（空）、“e”、“es” 和 “s”，从而得到单词 “car”、“care”、“cares” 和 “cars”。

此外：由于天真的字符串拼接很容易“意外地变成平方级复杂度”，我们还希望聪明地使用 `StringBuilder`，避免不必要的重复字符串拷贝。我们先看代码，再详细拆解其中的关键之处。

Listing 5.4 基于 trie（或图）的单词列表

```csharp
using System.Text;
sealed class WordGraph : IWords #A
{
	private Node root;
    public WordGraph(Node root)
    {
    	this.root = root;
    }
    private Node? Find(string prefix)
    {
    	Node? n = root;
        foreach (char c in prefix)
        {
            n = n.TryFollowEdge(c);
            if (n == null)
            	break;
        }
    	return n;
    }
    public bool Contains(string word) => Find(word)?.IsEnd ?? false;
    public IEnumerable StartsWith(string prefix)
    {
        Node? n = Find(prefix);
        return n == null ?
            Enumerable.Empty() :
        	Words(n, new StringBuilder(prefix));
    }
    private IEnumerable Words(Node n, StringBuilder prefix)
    {
        if (n.IsEnd)
        	yield return prefix.ToString();
        foreach (char c in n.EdgeLabels)
        {
            prefix.Append(c);
            foreach (var word in Words(n.FollowEdge(c), prefix))
            	yield return word;
            prefix.Remove(prefix.Length - 1, 1); #B
        }
    }
    public IEnumerator GetEnumerator() => StartsWith("").GetEnumerator(); #C
    IEnumerator IEnumerable.GetEnumerator() => GetEnumerator();
}
```

我们来分解一下：

- 我把这个类命名为 `WordGraph`，是因为正如我们很快会看到的，这些算法对任何无环单词图都适用，而不只是对 tries。
- `Find` 会尝试从根节点开始，沿着给定前缀的每一个字符对应的边往下走；如果途中缺少某条边，就说明图中没有以该前缀开头的单词。
- `Contains` 使用 `Find` 来查找整个单词。如果 `Find` 返回 null，则该单词不在列表中；否则，当且仅当该节点被标记为单词结尾时，这个单词才算在列表里。
- `StartsWith` 稍微复杂一点。如果 `Find` 返回 null，则列表中没有任何以该前缀开头的单词，我们可以直接返回一个空序列。否则，我们把这个前缀放入一个 `StringBuilder` 中，并用 `Words` 这个辅助方法枚举所有以该前缀开头的单词。
- `Words` 会先检查给定的前缀本身是否就是列表中的一个单词；例如，当我们被要求枚举以 “car” 开头的单词时，“car” 本身也应该被产出（yield）。然后我们依次沿着当前节点的每一条出边向外扩展并递归调用。这个算法要求在 `Words` 返回之前把 `StringBuilder` 恢复到调用 `Words` 之初的状态。

`Contains` 的性能如何？我们的 `SortedWordList` 实现达到了 O(m log n)，但在这里我们已经赢了！如果给定的单词长度为 m，那么 `TryFollowEdge` 最多被调用 m 次。由于 `TryFollowEdge` 是 O(1) 的，总时间代价就是 O(m)。

那 `StartsWith` 呢？同样的道理。`SortedWordList` 在找到第一个匹配项时需要 O(m log n) 的时间，然后对每个后续匹配项再花 O(m) 时间来检查前缀。而这个实现能在 O(m) 时间内找到图中所有后缀汇聚的那个点，然后为每个被产出的字符串再花 O(m) 的代价构造出来。

对数级性能通常已经可以接受，因为它增长得很慢，但显然 O(m) 的代价要优于 O(m log n)；trie 似乎明显优于有序列表。

不幸的是，在分析 trie 构建器时，我略过了一个重要问题：通过前缀去重，我们到底节省了多少内存？

### 使用 trie 我们到底省了多少内存？

trie 的空间性能如何？这完全取决于单词列表中有多少公共前缀！我把图 5.1 抄成了这里的图 5.4，它展示了在加入九个单词之后的那棵 trie：
图 5.4 一个包含九个单词的 trie：car、care、cares、cars、fir、fire、firer、firers、firs

![](asserts/05-04.png)

原始的单词列表有九个单词，共 38 个字符；构建出的 trie 上带字符的边只有 13 条，所以存储的字符数量只有原始列表字符总数的大约三分之一！这确实是一个相当惊人的缩减，但你或许也能看到这里有两个问题。

- 每条边的开销都非常巨大：每条边都指向一个节点，而每个节点本身又包含一个字典，这些加起来肯定不小。我们会在本章末尾回到这个问题。
- 看起来似乎有机会减少节点的数量。叶节点 5、6、12 和 13 都没有出边，而且都是“单词结尾”节点。显然，节点 3、4、9 和 11 的出边 “s” 完全可以都指向节点 5，这样我们就可以删掉节点 6、12 和 13，把节点数从 14 个减少到 11 个，这是一个相当可观的缩减。

也就是说，我们似乎应该能把那棵九单词的 trie 重写成这个在逻辑上等价的“单词图”（图 5.5）：
图 5.5 对 trie 的所有叶子进行去重后得到一个等价的单词图。

![](asserts/05-05.png)

前缀树会对前缀进行去重，但任何一个足够大的英文单词列表中都会存在数量巨大的公共后缀；难道我们不能同时对后缀也做去重，从而获得更大的收益吗？我们可以从两头入手攻击这个问题！

在本章后面我们会真的这么做，但在那之前，先更深入地挖一挖 trie 存在的问题。刚才的例子太小了，无法看出问题到底有多严重。如果我们用 ENABLE 单词列表来构建一棵 trie 又会怎样呢？

```csharp
List words = File.ReadAllLines("enable.txt").ToList();
Node n = TrieBuilder.MakeTrie(words);
```

我写了一些代码（这里略去不表）来收集这棵 trie 的统计数据；结果非常惊人。ENABLE 列表中有 172,820 个单词和 1,570,508 个字符，而生成的 trie 有 387,881 个节点。一方面，节点数量不到字符数量的四分之一，这很棒；看起来在大型单词列表上做前缀去重确实很划算。另一方面，其中有 111,507 个节点是完全相同的叶子节点——也就是说，它们是“单词结尾”节点，有入边但没有出边。我们为在逻辑上是同一个叶子节点的东西分配了数以百万计的字节！

尽管我们在搜索性能上挤出了一点小优势，但在实践中，相比于仅仅使用一个排序列表并忍受前缀重复的代价，我们几乎没有真正节省任何内存。

我们可以在解决这个问题的路上走得很远，但在那之前，让我们先稍作岔开，谈谈 trie 与理论计算机科学之间的联系。

## Trie 其实就是有限状态自动机

有限状态自动机（FSA）是一种用来形式化描述简单机器行为的方式。一个 FSA 有有限个内部状态和有限个输入；每当收到一个输入时，机器就会根据当前状态和这个输入转移到一个（可能不同的）状态。

自动售货机就是一个很好的例子。它有有限个内部状态，用来记录投入了哪些硬币，以及每种零食还剩多少。当你按下选择按钮时，它的行为取决于输入——也就是你按下的是哪个按钮——以及机器当前的内部状态。也就是说，你付的钱够不够，你选的那种零食还有没有？零食被吐出来之后，机器的状态也会随之更新。

我们可以把 trie 看成是一种 FSA。每个节点就是一个状态，从该节点发出的每条边上的标签就是一个输入，这个输入会导致状态转移到下一个状态。根节点是“起始状态”，双圆节点是“成功状态”（接受状态）。我们可以把 `Contains` 操作理解为在问：“这串输入能不能让机器从起始状态走到某个成功状态？”以这种方式被机器“接受”的所有字符串——也就是我们的单词列表——就叫做这台机器的“语言”（language）。

我们可以用这套新术语重新表述上一节中的观察。我们那棵有 14 个状态的 trie，是一个状态数多于必要值的 FSA。通过合并叶子节点，我们可以在保持其接受的语言不变的前提下，减少 FSA 中的状态数量。于是一个很自然的问题就出现了：给定一个 FSA，如何找到一个拥有尽可能少状态、但接受同样语言的 FSA？如果能做到这一点，我们就能对 trie 做优化。

自至少 20 世纪 50 年代以来，这个问题就被计算机科学家大量研究，已经有许多解决这一问题的算法。其中很多算法使用了一种“划分”（partitioning）策略：也就是说，它们把所有状态划分成若干互不相交的集合，使得一个集合中的状态不可能在其他集合中有任何功能上等价的状态。常见的第一步划分就是先把状态分成“成功状态”和“非成功状态”；任何一个成功状态都不可能被某个非成功状态替代，反之亦然。算法随后会不断细化划分，直到找出那些在任何方面都无法彼此区分、且含有多个状态的集合；这些状态就可以被去重合并。

听起来我们也许可以先为单词列表构建一棵 trie，然后再对它运行一个 FSA 最小化算法。这样就能得到在逻辑上仍然表示我们单词列表的、状态最少的图。虽然听起来挺直接，但这些划分算法的细节却并不简单。这些算法本身既不太容易实现，也可能产生不小的性能开销。

而且，如果我们面对的问题是：某棵给定的 trie 实在太大，以至于装不进一台内存受限的机器，那么“先建一棵 trie 再最小化它”的办法就根本行不通。

我们真正需要的是一种算法：它接收一个单词列表，并直接生成一个最小的 FSA，而不是先膨胀成一棵庞大的 trie 再去压缩。幸运的是，确实存在一个不错的算法，而且并不难说明和实现。

## 构建 DAWG

DAWG 指的是 Directed Acyclic Word Graph（有向无环单词图）。通过不仅对前缀，还对后缀做去重，我们不再得到一棵树；而是得到了一张不含环的图。你也会看到它被称为 Deterministic Acyclic Finite State Automata（确定性有向无环有限状态自动机，DAFSA），但我更喜欢 DAWG 这个说法，因为我是这么学的，因为它强调了我们是在存储“单词”，而且这个词说起来也更有趣。

这里要介绍的构建 DAWG 的算法是 Jan Dacuik、Bruce Watson 和 Richard Watson 在 1998 年发明的。这个算法的核心思想是：一次一个单词地构建一个“几乎最小”的 DAWG。随着算法的执行，对于除了最后加入的那个单词的节点以外的所有单词，DAWG 始终都保持尽可能小。每当一个新单词被加入时，算法就会对“前一个被加入的单词”的节点做优化，从而维持这样一个不变式：除了最后一个单词的节点之外，图对所有单词都是最小的。在列表中最后一个单词加入之后，我们再把它也修整到位，这样就得到了一个真正最小的 DAWG。

我在这里要介绍的这个版本要求按字母顺序加入单词；还有一个更复杂的版本允许以任意顺序加入单词，不过我不会再展开讨论。

> **注意：**
> 这个算法假定单词列表是按照“逐字符”的方式排序的，也就是所谓的“序数排序”（ordinal sorting）。如果你的单词列表中包含拉丁字母以外的字符，就要小心一些情况，比如德语字符 “ß” 和两个字符的组合 “ss”；在非序数排序中，它们会被认为相等，而在序数排序中则不相等。类似的情况还有很多！

我们通过一个例子来把握这个思路，然后再写代码。还是用刚才那个排序好的单词列表：

```csharp
List words = ["car", "care", "cares", "cars", "fir", "fire", "firer", "firers", "firs"];
```

我们跳过前面几步；在加入前四个单词之后，我们得到的图其实就是用“构建 trie 的算法”所能得到的那棵 trie。注意，此时图中表示前三个单词的那一部分已经是最小化的，但在添加完 “cars” 之后，我们又多出一个冗余节点；在图 5.6 中，节点 5 和节点 6 是等价的。这没有问题；算法保证图对除了“最后加入的单词 cars”之外的所有单词都是最小的，而节点 6 正好是这个单词的结尾节点，所以我们还暂时没有修理它。

图 5.6 我们已经添加了 car、care、cares 和 cars。除了 “cars” 之外，图的其他部分都是优化过的。节点 6 与节点 5 完全相同，但因为节点 6 属于 “cars”，所以我们还没处理它。

![](asserts/05-06.png)

当我们加入 “fir” 时，算法首先会分析前一个已加入的单词，从它最右边的节点开始，在图中向左移动。它注意到状态 6 可以被状态 5 替换，于是先修正那一部分，然后再加入 “fir”，得到图 5.7：

图 5.7 在加入 “fir” 之后，除了 “fir” 以外，图中对所有单词都已经优化完毕。 状态 9 本可以用状态 5 来替换，所以这张图对于最后那个单词来说还不是最优的。

![](asserts/05-07.png)

我们再次得到一张除了最后一个单词之外都是极小（最小化）的图。在下一步我们加入 “fire” 时，有一个细微之处，请看看你能不能发现它（图 5.8）：

图 5.8 在加入 “fire” 之后，你有没有注意到这张图里有什么古怪的地方？

![](asserts/05-08.png)

我之前说过，算法会在加入下一个单词之前，先把前一个单词“修正好”。在图 5.7 中，节点 9 在逻辑上和节点 5 是一样的，那为什么我们在“修正前一个单词”的时候，没有把节点 8 更新为指向节点 5 呢？

因为如果那样做，我们就没有地方可以添加节点 10，而不同时把 “carese” 也加入到单词列表里！换句话说，在图 5.8 中，在节点 10 被加入之后，节点 9 就不再等同于节点 5 了。过早地修改节点 8 将是一个错误。

记住，这个算法所维持的不变式是：除了最后一个已加入单词中的那些节点之外，整个图都是优化过的。我们刚刚加入了 “fire”，此时除了该单词对应的节点 7、8、9 和 10 之外，图中所有节点都已经优化好了。不变式得以保持，而且这个有限自动机
 所表示的语言仍然完全符合我们的预期。

当我们加入 “firer” 和 “firers” 时，同样的事情也会发生。（图 5.9）当我们沿着 “fire” 分支往下扩展 “firer” 和 “firers” 时，
图依旧对除了最后一个已加入单词中的那些节点以外的所有部分都是优化的，所以我们不会尝试去优化节点 7、8、9 或 10。

图 5.9 在加入 “firer” 和 “firers” 之后，“firers” 的那些节点仍然没有被优化。

![](asserts/05-09.png)

真正有趣的地方出现在加入 “firs” 的时候。在尝试加入它之前，我们会依次分析节点 12、11 和 10，寻找可能的优化机会。我们不需要检查节点 7、8 或 9，因为它们将会成为 “firs” 这条路径上的节点；作为最后一个已加入的单词，“firs” 中的节点暂时尚未被优化是没问题的。节点 12 可以被节点 5 替换，从而得到图 5.10 所示的图：

图 5.10 我们从前一个已加入单词的末尾开始，向后（向左）寻找优化机会，首先发现节点 12 是多余的；节点 11 应该改为连一条边到节点 5。

![](asserts/05-10.png)

但我们还没有结束！在那次优化之后，我们可以看到 节点 4 和节点 11 都是接受状态，并且都只有一条标记为 “s” 的边指向节点 5。它们是等价的，所以我们也可以去掉节点 11，从而得到图 5.11 所示的图：

图 5.11 我们现在已经把 “firers” 中 “ers” 那一部分完全优化好了，并且已经准备好要加入 “firs”。

![](asserts/05-11.png)

接着我们像往常一样加入 “firs”。同样地，“firs” 所在路径上的节点此时尚未被优化是可以接受的。在图 5.12 中，除了最后的节点 13 之外，其他一切都已经优化完毕，而节点 13 等价于节点 5。

图 5.12 “firs” 已经加入完成，除了节点 13 之外，其他部分都已经优化；节点 13 相对于节点 4 来说是多余的。

![](asserts/05-12.png)

现在最后一个单词已经加入完毕，我们知道 “firs” 不会再有任何进一步的扩展，因此对它进行优化是安全的。我们必须依次分析节点 13、9、8 和 7。我们发现节点 13 等价于节点 5，于是修正节点 9 的指向。除此之外，我们不能再做更多优化了。这个单词列表对应的最小 DAWG 如图 5.13 所示：

图 5.13 这是能够表示那九个单词的最小可能的 DAWG。

![](asserts/05-13.png)

图 5.4 中表示该单词列表的那棵字典树（trie）拥有 14 个节点。我们之前注意到，通过合并叶子节点可以把它缩减到 11 个节点，但这一次我们做得更好：还把节点 11 用节点 3 来替换了。最终得到的最小化 DAWG 只有 10 个节点，而这已经是识别这组单词的有限状态自动机（FSA）所能达到的最小规模了。在本章后面，我们会看看在更大的单词列表上，DAWG 相比字典树能带来多大的节省；现在，让我们来写一些代码，实现这个 DAWG 构建算法。

### 为构建 DAWG 需要解决哪些问题？

要实现这个算法，我们需要解决以下四个问题：

1. 给定一个新单词，比如 “firs”，要找出在图中，跟前一个已加入的单词 “firers” 的公共前缀之后的那个图节点。我们已经从构建字典树（trie）的算法中知道怎么做：从根开始依次沿着边 ‘f’、‘i’、‘r’ 走到节点 9。此时我们发现从那里找不到一条标记为 ‘s’ 的边，因此节点 9 就标识出了 “firs” 和 “firers” 的公共前缀。
2. 找出前一个已加入单词 “firers” 中，哪些节点需要被检查以寻找优化机会。由于单词是按排序顺序被加入的，我们可以从公共前缀节点出发，沿着从该节点发出的“最大”字母那条边，来找到前一个已加入单词的那些节点。我们需要写一些代码来完成这件事。
3. 既然已经知道要检查哪些节点，我们接下来需要发现它们当中是否有节点可以被之前已经优化好的节点替换，并且执行这一替换。这一步才是真正棘手的部分！我们需要高效且安全地完成它。
4. 最后，加入新的后缀。这一步和向字典树中添加后缀的算法是一样的。

要解决问题 2 和问题 3，我们需要在我们的 Node 类中增加一些功能。我已经在类的底部添加了三个新成员：

代码清单 5.5 为 Node 类添加三个新成员

```csharp
sealed class Node
{
    private readonly Dictionary<char, Node> edges = [];
    public bool IsEnd { get; set; }
    public IEnumerable<char> EdgeLabels => edges.Keys;
    public Node FollowEdge(char c) => edges[c];
    public Node? TryFollowEdge(char c) =>
        HasEdge(c) ? FollowEdge(c) : null;
    public bool HasEdge(char c) => edges.ContainsKey(c);
    public Node AddNewNode(char c) => edges[c] = new();
    public Node TryAddNewNode(char c) =>
        HasEdge(c) ? FollowEdge(c) : AddNewNode(c);
    public int Count => edges.Count; #A
    public char LastEdgeLabel => EdgeLabels.Max(); #B
    public Node ReplaceEdge(char c, Node n) => edges[c] = n; #C
}
```

- `Count` 返回从该节点出发的边的数量。
- `LastEdgeLabel` 返回从该节点出发的边中，字母值最高的那条边的标签；如果 `Count` 为零，那么这里会抛出异常。由于单词是按字母顺序被逐个加入的，从根开始一路沿着“最后一条边”就能找到最近加入单词的那些节点。
-  `ReplaceEdge` 会把一条已有的边重新指向给定的节点。

到这里，问题 2 已经解决了。那么我们要如何解决问题 3 呢？比如在图 5.10 中，我们如何发现节点 11 等价于节点 3 呢？

### 节点的等价性

我们需要一个清晰的“等价”定义，才能把它翻译成代码。两个图节点 x 和 y 当且仅当满足以下条件时是“等价”的：

- 它们要么都是单词结尾节点，要么都不是单词结尾节点：

  `x.IsEnd == y.IsEnd`

- 它们有相同数量的出边：
   `x.Count == y.Count`

- 它们这些出边上的标签集合相同。

- 这些出边引用的都是相同的节点：对所有出边标签 e，`x.FollowEdge[e]` 在引用上等同于 `y.FollowEdge[e]`。

这已经足以让我们开始写代码了。我们会像之前实现字典树构建器那样，实现一个 DAWG 构建器。把“等价性”的实现封装到一个实现了 `IEqualityComparer<Node>` 的对象里会很方便：

代码清单 5.6 使用相等比较器计算 Node 的等价性

```csharp
sealed class DawgBuilder
{
    private class NodeEquality : IEqualityComparer<Node>
    {
        public bool Equals(Node? x, Node? y)
        {
            if (ReferenceEquals(x, y))
                return true;
            if (ReferenceEquals(x, null) || ReferenceEquals(y,
                null))
                return false;
            if (x.IsEnd != y.IsEnd || x.Count != y.Count)
                return false;
            return x.EdgeLabels.All(e =>
                y.HasEdge(e) &&
                ReferenceEquals(x.FollowEdge(e),
                    y.FollowEdge(e)));
        }
```

在这段很短的代码中，有几件事需要注意：

- 最前面的几行是一些“快速退出”的情况；如果我们在比较引用相同的两个节点，它们显然是相等的。如果我们在比较一个非空节点和一个空引用，它们显然不相等。在这个算法里，这些条件都不会真正触发，但以这样的方式来写你的相等比较器是一个好习惯。
- 如果单词结束标记不同，或者边的数量不同，那么它们不可能是等价节点。
- 最后，我们检查 x 的所有边标签是否也都是 y 的边标签，并且这些边引用的节点在引用上是相同的。

实现一个相等比较器还要求我们同时实现 `GetHashCode`，而且稍后可以看到，我们确实需要它。编写哈希码时有一些重要的规则和指导原则：

- 如果你把一个可变对象存进哈希集合，或者把它用作字典的键，你的代码必须保证在它被存储期间，**不会**以任何会改变其哈希码的方式去修改这个对象。哈希码是哈希集合或字典用来在内部数据结构中定位该对象的方式；如果哈希码改变了，对象就可能变得无法被找到！由于我们选择了使用可变节点类型，并且我们在一个可以被修改的数据结构上计算等价性，我们必须小心地保证自己遵守这条规则。
- 如果两个对象是等价的，那么它们的哈希码必须相等。（反过来不一定成立：哈希码相同的对象是允许并非等价的。）哈希集合的算法要求等价对象必须有相同的哈希码。
- 如果你不是那种非常熟悉如何把多份数据组合成一个哈希码的专家，可以考虑使用 `HashCode` 类：对每一份你用来计算等价性的字段都调用一次 `Add`。注意调用 `Add` 的顺序是有影响的。

让我们用一个正确的 `GetHashCode` 实现来完成相等比较器：

代码清单 5.7 为 Node 的相等比较器补全 `GetHashCode` 实现

```csharp
        public int GetHashCode(Node n)
        {
            var h = new HashCode();
            h.Add(n.IsEnd);
            foreach(var e in n.EdgeLabels.OrderBy(c=>c))
            {
                h.Add(e);
                h.Add(n.FollowEdge(e)); #A
            }
            return h.ToHashCode();
        }
    }
}
```

这里有几个微妙的地方需要注意：

- 在计算等价性时我们用到了 `Count`，但在计算哈希码时却没有使用它。写成 `h.Add(n.Count)` 并不会是错的，但其实也没有必要。唯一的硬性要求是等价节点必须有相同的哈希码，而在不把 `Count` 用进哈希计算时，这个条件依然成立。
- 调用 `Add` 时，顺序很重要。我们在 `EdgeLabels` 的实现里并没有保证：对于拥有相同边标签集合的两个节点，总是以某个固定顺序返回边标签。因此，我们必须在对标签和节点做哈希之前，把这些边标签强制排序为一个标准顺序。
- 我们调用了 `Add(n.FollowEdge(e))`；你可能会担心这是否会递归地调用当前这个 `GetHashCode` 实现。答案是否定的；它调用的是 `Object.GetHashCode`，也就是对**节点引用**进行哈希，而不是对节点内容进行哈希。这正是我们想要的；我们是用引用比较来计算等价性的。

在解决问题 3 的路上我们已经取得了很大进展：现在我们可以拿两个节点进行等价性比较，这意味着我们可以识别出哪些节点可以被去重，从而最小化 DAWG。但我们要如何高效地做到这一点呢？我们会考虑许多可能需要去重的节点，如果每次都扫描整个图去找重复节点，那代价就会高得吓人。

你大概已经猜到解决方案了：我们将为已经优化过的图的那一部分中的“规范节点”（canonical nodes）构建一个哈希集合。由于哈希集合的查找时间是 O(1) 的，我们就可以使用刚刚写好的相等比较器，快速检查某个给定节点是否与某个等价节点匹配。唯一需要极其小心的是：一旦某个节点被加入了这个“规范集合”，它就决不能再被修改。如果它被修改了，它的哈希值也会改变，那么我们就无法再可靠地判断它是否在集合中。为此我们必须小心，只在节点已经完全优化并且不会再发生变化时，才把它加入规范集合。

我们已经有了一个扎实的方案来检测重复节点。现在剩下的工作，就是实现整个算法了。

### DAWG 构建算法

现在我们已经有了构建最小 DAWG 所需的工具，可以一次构建一个单词。我们必须先与前一个单词找到公共前缀，然后优化前一个单词的后缀，最后再为新单词添加后缀：

代码清单 5.8 DAWG 构建算法

```csharp
sealed class DawgBuilder
{
    private class NodeEquality : IEqualityComparer<Node> { … }
    private Node root = new();
    private HashSet<Node> canonicalNodes =
        new HashSet<Node>(new NodeEquality()); #A
    private DawgBuilder() { }
    public static Node MakeDawg(IEnumerable<string> words) #B
    {
        var db = new DawgBuilder();
        foreach (var word in words)
            db.Add(word); #C
        db.FixUpLastWord(db.root); #D
        return db.root;
    }
    private void Add(string word)
    {
        int i = 0;
        Node n = root;
        for (; i < word.Length; i += 1) #E
        {
            if (!n.HasEdge(word[i]))
                break;
            n = n.FollowEdge(word[i]);
        }
        FixUpLastWord(n); #F
        for (; i < word.Length; i += 1) #G
            n = n.AddNewNode(word[i]);
        n.IsEnd = true;
    }
    private void FixUpLastWord(Node n)
    {
        if (n.Count == 0) #H
            return;
        char c = n.LastEdgeLabel;
        Node s = n.FollowEdge(c); #I
        FixUpLastWord(s); #J
        Node? canonical;
        canonicalNodes.TryGetValue(s, out canonical); #K
        if (canonical == null)
            canonicalNodes.Add(s); #L
        else
            n.ReplaceEdge(c, canonical); #M
    }
}
```

`Add` 方法只是 `TrieBuilder` 中 `Add` 的一个更啰嗦的版本，因为它把“查找公共前缀”和“追加新后缀”这两步拆成了两个循环，而不是用 `TryAddNewNode` 一次完成二者；这样做的原因是我们需要对前一个已经添加的单词的后缀做优化。

`FixUpLastWord` 中发生了很多事情。我们从递归开始，这是因为先优化单词尾部的节点和边，可能会为更靠近单词中部的节点创造后续优化的机会。我们在图 5.10 中就看到了这一点：在发现节点 12 可以被节点 5 替换之前，我们还不知道节点 11 可以被节点 3 替换。只有在节点 12 被消除之后，节点 11 和节点 3 才变得等价。

`HashSet<T>` 的 `TryGetValue` 方法就是专门为我们现在遇到的问题设计的：我们有一组“规范（canonical）”对象，希望在其中找到一个与给定对象等价的规范对象；如果存在，就返回那个对象；如果不存在，则返回 `null`，表示没有这样的对象。

我们有没有遵守“在哈希集合中使用可变对象”的规则？答案是有的。当我们调用 `n.ReplaceEdge` 时，节点 `n` 还不在规范集合中，因此修改它没有问题。而递归调用已经确保在节点 `s` 被加入规范集合之前，对它所有必要的修改都已经完成；之后它就不会再改变了。

那么 `Contains` 和 `StartsWith` 的实现呢？这些代码我们已经写过并分析过了；我们可以直接重用 `WordGraph` 类，因为它对 DAWG 和对 trie 一样适用：

```csharp
Node dawg = DawgBuilder.MakeDawg(words);
WordGraph wg = new WordGraph(dawg);
```

这就是 DAWG 构建算法的全部内容。它确实比 trie 构建算法长得多，但在概念上并没有那么不同；它只是在人添加单词的过程中途做了一些优化工作而已。通过加入这些额外工作，我们是否在渐近复杂度上付出了额外代价呢？

### 一边构建一边优化图，会增加多少开销？

DAWG 构建算法和 trie 构建算法都会查找公共前缀，也都会为新的后缀创建新节点。DAWG 构建器多了一步：对前一个添加的单词做优化。因此，从“墙上时钟”（实际运行时间）的角度看，DAWG 构建器总是要更慢一些。但是，它的渐近复杂度有没有提升呢？trie 构建器的时间复杂度是 O(mn)——也就是对单词表中字符总数的线性复杂度。DAWG 构建器会比这差多少？

DAWG 构建器同样有对每个单词的每个字母进行处理的 O(mn) 成本，然后再额外付出调用 `FixUpLastWord` 的开销。`FixUpLastWord` 会被调用多少次？每次我们添加新单词时，都要优化前一个添加的单词的某一部分，然后所有被优化过的节点就变成了规范节点，之后再也不会被优化。因此，`FixUpLastWord` 对每个添加的节点只调用一次，总调用次数少于 mn。

这个方法具体做了什么？它的所有组成部分都很便宜：查找最后一条边的标签、在规范集合中查找、向集合中添加新元素以及修改一条边，这些操作的复杂度都是 O(1)。所有对 `FixUpLastWord` 的调用加在一起，也不会比 O(mn) 更贵，所以我们可以得出结论：DAWG 构建器的渐近复杂度同样是 O(mn)。在渐近意义上，它并不比 trie 构建器更糟。

>  **注意：**
> 我们调用 `Max()` 来找到最后一条边的标签；这不是 O(n) 吗？是的，但这里 n 的最大值是字母表的大小，这是一个很小的常数。而且在实际中，大多数节点的出边数量都很少，远远小于字母表的大小。

那内存呢？DAWG 构建器看上去比 trie 构建器使用了更多内存，因为它维护了一组规范节点，但这组集合的大小永远不会超过最终 DAWG 的大小。构建 DAWG 的全部意义就在于：最终 DAWG 的大小会远小于等价的 trie！因此，相比 trie，DAWG 在空间复杂度上并没有增加。

在实践中，对于一个典型的基准测试，我们到底能节省多少字节呢？

## 针对 ENABLE 的 DAWG 与 trie 对比

在 ENABLE 基准测试中，有 172,820 个单词、1,570,508 个字符。对这个单词列表，用 trie 和 DAWG 的节点数量做对比，可以看出在英文词典中进行后缀去重的威力：

表 5.1 按出边数量比较节点数

|                |            |            |
| -------------- | ---------- | ---------- |
| Outgoing edges | Trie nodes | DAWG nodes |
| 0              | 111,507    | 1          |
| 1              | 214,096    | 25,921     |
| 2              | 39,181 12  | 840        |
| 3              | 13,525     | 6.755      |
| 4              | 4,490      | 3,618      |
| 5              | 1,994      | 1,944      |
| 6 - 26         | 3088       | 3088       |
| Total nodes    | 387,881    | 54,167     |
| Total edges    | 387,880    | 122,975    |

在表 5.1 中，就节点数而言，DAWG 比 trie 小了 86%。

注意到 trie 中有十多万个冗余叶子节点，这正是最初促使我们实现图结构的动机，所以仅仅通过把除了一个以外的所有叶子都消掉，就获得巨大收益，这并不让人意外。但对短后缀做去重带来的收益同样是巨大的；很多英文单词都以常见后缀结尾，比如 -s、-ed 或 -ing。其净效果是：表示 ENABLE 单词表的 DAWG 只拥有对应 trie 14% 的节点数，这个收益相当可观。

我们还可以看到，DAWG 的总边数也小得多，尽管它的减少幅度没有节点数那么戏剧性。可以这样来思考：对于一个 trie，或者说任何树结构，平均下来每个节点的入边数一定略小于 1，因为在任何一棵树中，除了根节点以外，每个节点恰好有一条入边。在构建 DAWG 时，我们进行的优化是：增加“规范（canonical）”节点的入边数量，同时删除冗余节点，因此剩余节点的平均入边数必然会上升。

那总字节开销呢？我们之前论证过，“单词列表”方案如果直接用现成的字符串，大约要消耗 8MB 左右；如果改为构建一个大的 UTF-8 字节数组，再做一个自定义字符串列表，列表中只存储该数组的偏移量，那么可以将总开销降到大约 2～3MB。那我们这里的 DAWG 方案，字节成本是多少？

我们有 5.4 万个 `Node` 对象，每一个都包含一个字典对象，而字典内部又有数组、链表等等各种结构。仅仅存储这些对象的引用就要超过 1MB 内存，而且每个对象本身内部还要存 16 字节的信息。综合来看，我们这里给出的 DAWG 方案所消耗的内存少于那个天真的字符串列表方案，但要多于使用大字节数组的自定义字符串列表方案。

在现代具有虚拟内存和垃圾回收的机器上，几十万个对象占用几兆字节的内存，对运行时来说根本不算什么压力；我完全愿意在一台现代机器上运行的真实程序中使用这里给出的方案。但如果我们真的必须进一步节省内存呢？要么因为内存非常紧张，要么因为 DAWG 要大得多？当年游戏开发者在只有 640KB 内存的机器上写英文单词游戏，却仍然能存下十多万个单词的单词表；在内存还又贵又少的年代，他们是怎么做到的？

一个常见的技巧是：分配一个大的字节数组，用来存放所有的节点和边。这个字节数组的布局是：先是一个节点的四字节头部，后面紧跟一串三字节偏移量，每条边对应一个偏移量，然后再是下一个节点的四字节头部，如此往复。这个四字节的头部中，有一位用作“是否为单词结尾”的标志位，还有若干位用来表示字母表中哪一个字母被用作了某条出边的标签。出边数量可以通过头部中“字母表部分”里 1 的位数来计算，而紧跟在头部后面的那一串三字节偏移量，则给出了表示这些边的节点头部，在数组中的偏移位置。

使用这样一种方案，我们可以把 54,167 个节点按每个 4 字节，122,975 条边按每条 3 字节存入一个 585,593 字节长的数组中，这完全落在老式 MS-DOS 机器 640K 的限制之内。即便在内存极其紧缺的环境中，DAWG 也能提供优雅、高效且易于搜索的优秀组合。

## 小结

 在一个列表中存储 n 个平均长度为 m 的单词，并且要支持根据某个前缀生成所有匹配的单词，这是一个常见的问题。单词的数量可以非常大。

哈希集合非常擅长判断某个单词是否包含在集合中，但在给定前缀的情况下生成所有补全结果，这方面就完全不行。

我们可以通过在有序列表上做二分搜索获得 O(m log n) 的搜索性能，这大概率是可以接受的——但如果使用现成的组件来实现，这个方案会用掉出乎意料的大量内存。

我们可以通过使用 trie 对前缀去重，从而获得 O(m) 的搜索性能，但冗余节点的数量可能会极其庞大。

通过注意到 trie 是有限状态自动机（FSA）的一种特例，我们可以使用许多可用的算法来最小化 FSA，同时保持其识别的语言不变。但是，先在内存中构建一个庞大且空间效率极低的 trie，然后再运行一个昂贵而复杂的最小化算法，并不是一个好主意；我们真正想要的是：一次添加一个单词，直接构建出一个已经优化好的 DAWG。

在图最小化过程中，高效地检测何时两个节点是冗余的、可以安全合并，是一个关键子问题；如果我们小心避免在某个节点进入规范节点集合之后再去修改它，就可以非常高效地完成这件事。

如果我们需要，完全可以把 DAWG 做得非常省内存；当年游戏开发者就是这样做的。