# 大语言模型：从理论到实践

## Transformer 模型

Transformer 架构是一种基于自注意力机制（self-attention）的深度学习模型，最初由 Vaswani 等人在 2017 年提出，并在其论文["Attention is All You Need"](https://arxiv.org/abs/1706.03762)中进行了详细描述。它在处理序列数据时避免了传统的循环神经网络（RNN）结构，而是采用了一种全新的架构。

Transformer 架构主要由两个部分组成：编码器（Encoder）和解码器（Decoder）。

1. 编码器（Encoder）：
   - 输入嵌入（Input Embedding）：将输入序列中的每个单词或标记转换为向量表示。
   - 位置编码（Positional Encoding）：为了捕捉输入序列中的位置信息，为每个输入位置提供一个位置编码，它包含了关于单词在序列中位置的信息。
   - 自注意力机制层（Self-Attention Layer）：自注意力机制允许模型在编码每个输入词时对其它输入词进行加权关注，从而捕捉到输入序列中的全局上下文信息。
   - 前馈神经网络层（Feed-Forward Neural Network Layer）：对自注意力机制的输出进行非线性变换和特征提取。
2. 解码器（Decoder）：
   - 目标嵌入（Target Embedding）：将目标序列的每个单词或标记转换为向量表示。
   - 位置编码（Positional Encoding）：与编码器相同，为了捕捉目标序列中的位置信息，为每个目标位置提供一个位置编码。
   - 自注意力机制层（Self-Attention Layer）：类似于编码器，解码器也使用自注意力机制来逐步生成目标序列，**每个位置的生成依赖于前面已经生成的部分。**
   - 编码器-解码器注意力机制层（Encoder-Decoder Attention Layer）：解码器通过编码器-解码器注意力机制来关注输入序列的不同位置以获取上下文信息。
   - 前馈神经网络层（Feed-Forward Neural Network Layer）：与编码器相同，对自注意力机制和编码器-解码器注意力机制的输出进行非线性变换和特征提取。
   - 输出层（Output Layer）：通过线性变换和 Softmax 函数将解码器的隐藏状态映射到目标序列的概率分布，从中选择最可能的单词作为当前位置的输出。

> 在编码器和解码器的结构中，主要涉及到四个模块：
>
> 1. 注意力层：使用多头注意力（Multi-Head Attention）机制整合上下文语义，它使得序列中任意两个单词之间的依赖关系可以直接被建模而不基于传统的循环结构，从而更好地解决文本的长程依赖。
> 2. 位置感知前馈层（Position-wise FFN）：通过全连接层对输入文本序列中的每个单词表示进行更复杂的变换。
> 3. 残差连接：对应图中的 Add 部分。它是一条分别作用在上述两个子层当中的直连通路，被用于连接它们的输入与输出。从而使得信息流动更加高效，有利于模型的优化。具体来说，在编码器和解码器的每个子层中，输入通过一个残差连接直接添加到该子层的输出上。这样可以保留原始输入的信息，并允许梯度更容易地反向传播。
> 4. 层化归一：对应图中的 Norm 部分。作用于上述两个子层的输出表示序列中，对表示序列进行层归一化操作，并对每个特征维度进行标准化。这种标准化操作有助于缓解梯度消失和梯度爆炸问题，并提高模型的训练稳定性。具体来说，对于每个子层的输入，层归一化通过减去均值、除以标准差，并使用可学习的参数进行缩放和平移。

Transformer 架构通过自注意力机制和前馈神经网络的组合，实现了对输入序列的编码和对输出序列的解码。它在自然语言处理任务中取得了很大的成功，并且已经成为该领域的重要基础模型。由于并行计算的能力，Transformer 架构还具备较高的计算效率。

![](./asserts/4.png)

图：基于transformer的编码器和解码器结构

从上图我们也可以了解到输入输出的工作流程：

- 输入
  - 输入序列编码：将输入序列中的每个单词或标记转换为向量表示。这可以通过使用嵌入层（Embedding Layer）来完成，其中每个单词被映射到一个实数向量。
  - 位置编码：为了捕捉输入序列中的位置信息，需要为每个输入位置提供一个位置编码。位置编码是一个与单词嵌入具有相同维度的向量，它包含了关于单词在序列中位置的信息。
  - 输入编码器：经过位置编码后，输入序列被送入多个编码器层。每个编码器层由自注意力机制（Self-Attention Mechanism）和前馈神经网络（Feed-Forward Neural Network Layer）组成。自注意力机制用于对输入序列中的单词进行加权关注，以获取全局上下文信息。前馈神经网络用于对加权后的信息进行非线性变换和特征提取。
- 输出
  - 解码器输入：解码器的输入是目标序列的前一个单词以及编码器的输出。类似于编码器输入，目标序列的单词也被嵌入为向量表示，并进行位置编码。
  - 解码器层：解码器由多个解码器层组成，每个解码器层包含自注意力机制、编码器-解码器注意力机制（用于关注输入序列的不同位置）和前馈神经网络。自注意力机制用于生成目标序列中的每个位置，编码器-解码器注意力机制用于关注输入序列的不同位置以获取上下文信息，前馈神经网络用于特征提取和非线性变换。
  - 输出层：解码器的最后一层通过线性变换和 Softmax 函数将解码器的隐藏状态映射到目标序列的概率分布。根据该概率分布，可以选择具有最高概率的单词作为当前位置的输出，并将其作为下一个解码器输入的一部分。

## 基于 HuggingFace 的预训练语言模型实践

- 安装 HuggingFace Transformer 库。

  ```
  pip install transformer
  ```

- 加载预训练语言模型（BEAT，GPT等）。

- 准备数据集，可以在 Dataset 库中下载常见的大规模数据集。

- 训练词元分析器（Tokenizer），通过分词技术（BERT 采用 WordPiece 分词）；根据训练语料中的词频决定是否将一个完整的词
  切分为多个词元。

- 预处理语料集合，将训练好的词元分析器进行处理，如文档长度超过一定数量，就会直接进行截断处理。

- 模型训练。

- 模型使用。

## LLaMA 模型

LLaMA 也是用的 Transformer 架构。与前面讲的 Transformer 架构不同的地方包括采用了**前置层归一化（Pre-normalization）**并使用 **RMSNorm归一化函数（NormalizingFunction）**、激活函数更换为 SwiGLU，并使用了**旋转位置嵌入（Rotary Positional Embeddings,RoP）**。整体 Transformer 架构与 GPT-2 类似：

![](./asserts/5.png)

图：GPT-2 模型架构

