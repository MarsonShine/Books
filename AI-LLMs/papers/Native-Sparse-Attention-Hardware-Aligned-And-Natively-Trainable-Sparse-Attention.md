# 原生稀疏注意力：与硬件对齐且原生可训练的稀疏注意力

> 下面主要涉及到几个技术术语，为了理解我将这几个技术点坐下简要说明：
>
> 全注意力（Full Attention）、稀疏注意力（Sparse Attention）、压缩注意力（Compressed Attention）、选择性注意力（Selective Attention）和上下文滑动注意力（Contextual Sliding Attention）是近年来在深度学习领域，尤其是在自然语言处理（NLP）和计算机视觉（CV）任务中，用来改进传统注意力机制的不同技术或方法。每种方法旨在提高模型的效率和性能，尤其是在处理长序列数据时。
>
> ### 1. **全注意力 (Full Attention)**
>
> 全注意力机制（通常被称为“自注意力”或“Transformer注意力”）是指每个输入序列中的每个元素（例如，单词、词向量、图像块等）都与其他所有元素进行互动，计算出一组权重来衡量其与其他元素的相关性。在这种机制下，序列中的每一对元素都会计算一次注意力值，从而生成最终的表示。
>
> - **优点**：准确捕捉全局依赖关系。
> - **缺点**：计算复杂度高（时间复杂度为O(n²)，n是序列长度），对于长序列效率低下。
>
> ### 2. **稀疏注意力 (Sparse Attention)**
>
> 稀疏注意力是一种对全注意力的改进，旨在减少计算复杂度。它通过只关注序列中某些位置的元素之间的关系，而不是计算所有可能的元素对之间的关系，从而节省了计算资源。这可以通过以下方式实现：
>
> - **固定模式稀疏注意力**：例如，通过窗口化的注意力机制，只关注每个元素的局部邻域。
> - **学习型稀疏注意力**：通过学习哪些元素对应该被关注，例如，使用稀疏矩阵表示注意力权重。
> - **优点**：降低计算复杂度，能够处理更长的序列。
> - **缺点**：可能会丧失一些长距离依赖关系。
>
> ### 3. **压缩注意力 (Compressed Attention)**
>
> 压缩注意力是一种通过压缩输入序列的表示来减少计算复杂度的方法。在这种方法中，通过将输入序列的某些部分进行压缩或聚合，然后再进行注意力计算，从而减少需要处理的元素数量。压缩可以通过降维、量化或其他压缩技术来实现。
>
> - **优点**：有效降低计算和存储需求。
> - **缺点**：可能会损失信息，从而影响模型的性能。
>
> ### 4. **选择性注意力 (Selective Attention)**
>
> 选择性注意力指的是模型只关注输入序列中的某些关键部分或特征，而不是处理所有输入。这些关键部分通常是由某种机制（如学习的门控机制、信号强度等）选择出来的。选择性注意力有助于减少不必要的计算，同时保持对关键特征的关注。
>
> - **优点**：提高计算效率，聚焦于关键信息。
> - **缺点**：选择机制可能不完美，导致重要信息被忽视。
>
> ### 5. **上下文滑动注意力 (Contextual Sliding Attention)**
>
> 上下文滑动注意力是一种结合局部窗口和上下文信息的注意力机制。在这种机制中，模型在处理序列时，会根据滑动窗口的大小，将注意力局限于一定范围内的上下文，并逐步滑动这个窗口来捕捉更大的上下文信息。这个方法通过避免全序列之间的全连接计算来降低计算复杂度。
>
> - **优点**：能够有效处理长序列，同时保持全局上下文。
> - **缺点**：窗口大小的选择可能影响模型的性能，过小可能丧失全局信息，过大可能导致计算量过大。

## 摘要

长上下文建模对于下一代语言模型至关重要，但标准注意力机制的高计算成本带来了巨大的计算挑战。稀疏注意力提供了一种有前景的方向，可以在保持模型能力的同时提高效率。我们提出了NSA，一种原生可训练的稀疏注意力机制，它结合了算法创新和与硬件对齐的优化，从而实现高效的长上下文建模。NSA采用了一种动态分层稀疏策略，将粗粒度的标记压缩与精细粒度的标记选择相结合，以保留全局上下文感知和局部精度。我们的方法通过两项关键创新推动了稀疏注意力设计的进步：(1) 通过算术强度平衡的算法设计实现了显著的加速，并对现代硬件进行了优化；(2) 我们支持端到端训练，减少了预训练计算，而不牺牲模型性能。图1实验结果显示，使用NSA预训练的模型在通用基准、长上下文任务和基于指令的推理任务上，性能与全注意力模型相当或更好。同时，在64k长度的序列处理上，NSA在解码、前向传播和反向传播阶段显著加速，验证了其在模型生命周期中的效率。

## 1 引言

![](../asserts/x1.png)

**图1：**全注意力模型与NSA之间的性能和效率比较。左：尽管是稀疏的，NSA在一般基准、长上下文任务和推理评估中，平均超越了全注意力基准。右：在处理64k长度序列时，NSA在所有阶段（解码、前向传播和反向传播）相比全注意力实现了显著的计算加速。

随着现实世界应用的不断发展，研究界越来越认识到长上下文建模是下一代大型语言模型的关键能力，应用场景包括深度推理（[Zelikman et al., 2022](https://arxiv.org/html/2502.11089v1#bib.bib33)；[DeepSeek-AI, 2025](https://arxiv.org/html/2502.11089v1#bib.bib9)）、库级代码生成（[Zhang et al., 2023a](https://arxiv.org/html/2502.11089v1#bib.bib34)）和多轮自主智能体系统（[Park et al., 2023](https://arxiv.org/html/2502.11089v1#bib.bib21)）。近期的突破包括OpenAI的o系列模型、DeepSeek-R1（[DeepSeek-AI, 2025](https://arxiv.org/html/2502.11089v1#bib.bib9)）和Gemini 1.5 Pro（[Google et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib13)），这些模型能够处理整个代码库、长文档，维持数千个标记的连贯多轮对话，并进行复杂的跨长范围依赖推理。然而，原生注意力（[Vaswani et al., 2017](https://arxiv.org/html/2502.11089v1#bib.bib27)）机制的高复杂性（[Zaheer et al., 2020](https://arxiv.org/html/2502.11089v1#bib.bib32)）在序列长度增加时成为了一个关键的延迟瓶颈。理论估计表明，在解码64k长度上下文时，基于softmax的注意力计算占总延迟的70-80%，这凸显了对更高效注意力机制的迫切需求。

高效长上下文建模的自然方法是利用softmax注意力的固有稀疏性（[Ge et al., 2023](https://arxiv.org/html/2502.11089v1#bib.bib12)；[Jiang et al., 2023](https://arxiv.org/html/2502.11089v1#bib.bib15)），通过选择性地计算关键的查询-键对，可以显著减少计算开销，同时保持性能。最近的进展通过各种策略证明了这一潜力，包括KV缓存驱逐方法（[Zhang et al., 2023b](https://arxiv.org/html/2502.11089v1#bib.bib36)；[Li et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib19)；[Zhou et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib37)）、基于区块的KV缓存选择方法（[Tang et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib25)；[Xiao et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib30)）以及基于采样、聚类或哈希的选择方法（[Chen et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib5)；[Liu et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib20)；[Desai et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib10)）。尽管这些策略具有前景，现有的稀疏注意力方法在实际部署中往往表现不佳。许多方法未能达到与理论增益相当的加速；此外，大多数方法主要集中在推理阶段，缺乏有效的训练时支持，以充分利用注意力的稀疏性模式。

为了应对这些限制，部署有效的稀疏注意力需要解决两个关键挑战：(1) **硬件对齐的推理加速**：将理论上的计算减少转化为实际的速度提升，需要在预填充和解码阶段进行硬件友好的算法设计，以缓解内存访问和硬件调度瓶颈；(2) **训练感知的算法设计**：通过可训练的操作符实现端到端计算，减少训练成本，同时保持模型性能。这些需求对于现实世界的应用至关重要，以实现快速的长上下文推理或训练。在考虑这两个方面时，现有方法仍然存在明显的差距。

为了实现更有效和高效的稀疏注意力，我们提出了NSA，一种原生可训练的稀疏注意力架构，整合了分层的标记建模。**图2**展示了NSA如何通过组织键和值到时间块，并通过三条注意力路径进行处理：压缩的粗粒度标记、选择保留的精细粒度标记和用于本地上下文信息的滑动窗口。然后，我们实现了专门的内核，以最大化其实用效率。NSA引入了两个核心创新，分别对应上述关键需求：(1) 硬件对齐系统：优化基于块的稀疏注意力，以便充分利用Tensor Core并提高内存访问效率，确保算术强度平衡；(2) 训练感知设计：通过高效的算法和反向操作符，支持稳定的端到端训练。此优化使得NSA能够同时支持高效的部署和端到端训练。

我们通过在现实世界语言语料库上进行综合实验，评估了NSA的表现。在27B参数的Transformer骨架上预训练260B标记，并在通用语言评估、长上下文评估和思维链推理评估上评估NSA的性能。我们还与优化后的Triton实现（[Tillet et al., 2019](https://arxiv.org/html/2502.11089v1#bib.bib26)）在A100 GPU上的内核速度进行了比较。实验结果表明，NSA在性能上与全注意力基准相当或更优，同时超越了现有的稀疏注意力方法。此外，NSA在解码、前向和反向阶段相比全注意力实现了显著加速，且加速比在更长序列中进一步增加。这些结果验证了我们的分层稀疏注意力设计能够有效平衡模型能力和计算效率。

## 2 重新思考稀疏注意力方法

现代稀疏注意力方法在降低Transformer模型的理论计算复杂度方面取得了显著进展。然而，大多数方法主要在推理阶段应用稀疏性，同时保留预训练的全注意力骨架，这可能引入架构偏差，限制其充分利用稀疏注意力的优势。在介绍我们原生稀疏架构之前，我们通过两个关键视角系统地分析这些限制。

![](../asserts/x2.png)

**图2：** NSA架构概述。左：框架通过三条并行的注意力分支处理输入序列：对于给定的查询，先前的键和值被处理为粗粒度模式的压缩注意力、重要标记块的选择性注意力和用于本地上下文的滑动注意力。右：可视化每个分支产生的不同注意力模式。绿色区域表示需要计算注意力分数的区域，白色区域表示可以跳过的区域。

### 2.1 高效推理的假象

尽管在注意力计算中实现了稀疏性，许多方法未能在推理延迟中实现相应的减少，主要面临以下两个挑战：

#### 限制阶段的稀疏性

像H2O（[Zhang et al., 2023b](https://arxiv.org/html/2502.11089v1#bib.bib36)）这样的算法在自回归解码期间应用稀疏性，但在预填充阶段（如计算注意力图、索引构建）需要大量计算，导致推理阶段的加速未能在所有阶段得以实现。与此相比，像MInference（[Jiang et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib16)）这样的算法仅专注于预填充稀疏性。虽然这些方法在某些阶段无法加速，但它们在预填充主导的任务（如书籍摘要和代码补全）或解码主导的任务（如长链推理）中减少了推理速度。

#### 与先进的注意力架构的不兼容性

一些稀疏注意力方法未能适应现代解码高效架构，例如多查询注意力（MQA）（[Shazeer, 2019](https://arxiv.org/html/2502.11089v1#bib.bib23)）和分组查询注意力（GQA）（[Ainslie et al., 2023](https://arxiv.org/html/2502.11089v1#bib.bib1)）。这些架构通过共享键值对（KV）在多个查询头之间，显著减少了解码期间的内存访问瓶颈。然而，像Quest（[Tang et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib25)）这样的算法中，每个注意力头独立选择KV缓存子集，这种方式虽然能够减少计算操作，但在GQA架构下，KV缓存的内存访问量较高。这种架构特性意味着，这些方法虽然可以减少计算，但其内存访问模式与现代架构中的高效内存访问设计发生冲突。

这些限制源于现有稀疏注意力方法主要集中在KV缓存的减少或理论计算的减少上，但在先进框架或后端中难以实现显著的延迟减少。这促使我们开发结合先进架构和硬件高效实现的算法，以充分利用稀疏性，提升模型效率。

### 2.2 可训练稀疏性的神话

我们追求原生可训练稀疏注意力的动机来自对仅推理方法的分析所得出的两个关键洞察：(1) 性能退化：后期应用稀疏性迫使模型偏离其预训练优化轨迹。正如Chen等（[2024](https://arxiv.org/html/2502.11089v1#bib.bib5)）所证明的，前20%的注意力仅能覆盖70%的总注意力分数，这使得预训练模型中的检索头在推理过程中容易被剪枝。(2) 训练效率需求：高效处理长序列训练对于现代LLM开发至关重要，包括在长文档上的预训练以增强模型能力，以及随后的适应阶段，如长上下文微调和强化学习。然而，现有的稀疏注意力方法主要针对推理，训练中的计算挑战尚未得到有效解决。这一限制妨碍了通过高效训练开发更强大的长上下文模型。

### 2.3 原生稀疏性的重要性

推理效率和训练可行性中的这些限制促使我们对稀疏注意力机制进行了根本性重设计。我们提出了NSA，这是一种原生稀疏的注意力框架，解决了计算效率和训练需求。在接下来的部分中，我们将详细介绍NSA的算法设计和操作符实现。

## 3 方法论

我们的技术方法涵盖了算法设计和内核优化。在以下小节中，我们首先介绍我们的技术背景。然后，我们展示NSA的整体框架及其关键算法组件，最后详细说明硬件优化的内核设计，以最大化实际效率。

### 3.1 背景

注意力机制广泛应用于语言建模中，其中每个查询标记 $𝐪_t$ 会计算与所有前面的键 $𝐤_{:t}$ 之间的相关性得分，以生成值 $𝐯_{:t}$ 的加权和。正式地，对于长度为 t 的输入序列，注意力操作定义为：

![](../asserts/nsa-math1.png)

其中，Attn 表示注意力函数：

![](../asserts/nsa-math2.png)

这里，$α_{t,i}$ 表示查询 $𝐪_t$ 和键 $𝐤_i$ 之间的注意力权重，$d_k$ 是键的特征维度。随着序列长度的增加，注意力计算在整体计算成本中占据越来越重要的地位，给长上下文处理带来了显著的挑战。

**算术强度(Arithmetic Intensity)**是计算操作与内存访问的比率。它在本质上决定了硬件上的算法优化。每个 GPU 都有一个关键的算术强度，这个强度由其峰值计算能力和内存带宽决定，计算公式为这两个硬件限制的比率。对于计算任务，算术强度高于该临界阈值时，变成计算限制（由 GPU 的浮点运算能力 FLOPS 限制），低于该阈值时，则变成内存限制（由内存带宽限制）。

对于因果自注意力机制，在训练和预填充阶段，批处理的矩阵乘法和注意力计算表现出较高的算术强度，使得这些阶段在现代加速器上是计算限制的。相比之下，自回归解码则成为内存带宽受限的，因为它每次前向传播生成一个标记，同时需要加载整个键值缓存，导致算术强度较低。这就导致了不同的优化目标——在训练和预填充阶段减少计算成本，在解码阶段减少内存访问。

### 3.2 总体框架

为了利用具有自然稀疏模式的注意力潜力，我们提出用更紧凑和信息密集的表示键值对 $K_{:t}, V_{:t}$ 替代方程式 1 中的原始键值对 $𝐤_{:t}, 𝐯_{:t}$，针对每个查询 $𝐪_t$。具体地，我们正式定义优化后的注意力输出如下：

![](../asserts/nsa-math3~4.png)

其中，$\tilde{K}_t, \tilde{V}_t$ 是根据当前查询 $𝐪_t$ 和上下文记忆 $𝐤_{:t}, 𝐯_{:t}$ 动态构建的。我们可以设计不同的映射策略，以获得不同类别的 $\tilde{K}_t^c, \tilde{V}_t^c$，并将它们组合如下：

![](../asserts/nsa-math5.png)

如 **图2** 所示，NSA 有三种映射策略 $𝒞=\{cmp, slc, win\}$，分别代表压缩、选择和滑动窗口用于键和值。$g_t^c∈[0, 1]$ 是对应策略 c 的门控得分，通过 MLP 和 Sigmoid 激活函数从输入特征中获得。让 $N_t$ 表示重映射键/值的总数：

![](../asserts/nsa-math6.png)

我们通过确保 $N_t ≪ t$ 来保持较高的稀疏比。

### 3.3 算法设计

在本小节中，我们介绍了我们的重映射策略 $f_K$ 和 $f_V$ 的设计：标记压缩、标记选择和滑动窗口。

#### 3.3.1 标记压缩

通过将键或值的顺序块聚合为块级表示，我们获得了压缩的键和值，捕获了整个块的信息。正式地，压缩键表示定义为：

![](../asserts/nsa-math7.png)

其中，l 是块长度，d 是相邻块之间的滑动步幅，φ 是一个可学习的 MLP，具有块内位置编码，将块内的键映射为一个压缩键。$\tilde{K}_t^{cmp}∈ℝ^{d_k×⌊\frac{t-l}{d}⌋}$ 是由压缩键组成的张量。通常，我们采用 $d < l$ 也有类似的公式。压缩表示捕获了更粗粒度的高层语义信息，并减少了注意力计算的负担。

#### 3.3.2 标记选择

仅使用压缩的键时，值可能会丢失重要的细粒度信息，这促使我们有选择性地保留单个键和值。下面我们描述我们的高效标记选择机制，该机制能够识别并保留最相关的标记，同时降低计算开销。

**基于块的选择**。我们的选择策略在空间上连续的块中处理键和值序列，这受到两个关键因素的启发：硬件效率考虑和注意力得分的固有分布模式。**基于块的选择对于在现代 GPU 上实现高效计算至关重要**。这是因为现代 GPU 架构对于连续块访问相比随机索引读取具有显著更高的吞吐量。此外，基于块的计算能够最优利用 Tensor Cores。这个架构特性使得基于块的内存访问和计算成为高性能注意力实现的基本原则，例如 FlashAttention 的基于块的设计。**基于块的选择遵循了注意力得分的固有分布模式**。之前的研究（[Jiang et al., 2024](https://arxiv.org/html/2502.11089v1#bib.bib16)）表明，注意力得分通常呈现空间连续性，这表明相邻的键往往具有相似的重要性级别。我们在第 6.2 节的可视化也展示了这一空间连续模式。

为了实现基于块的选择，我们首先将键和值序列划分为选择块。为了识别对注意力计算最重要的块，我们需要为每个块分配重要性得分。以下是我们计算这些块级重要性得分的方法。

**重要性得分计算**。计算块的重要性得分可能会引入显著的开销。幸运的是，压缩标记的注意力计算产生了中间注意力得分，我们可以利用这些得分来诱导选择块的重要性得分，公式为：

![](../asserts/nsa-math8.png)

其中，$𝐩_t^{cmp}∈ℝ⌊\frac{t-l}{d}⌋$ 是查询 $𝐪_t$ 与压缩键 $\tilde{K}_t^{cmp}$ 之间的注意力得分。设 $l′$ 为选择块的大小。当压缩块和选择块共享相同的分块方案，即 $l′=l=d$ 时，我们可以直接通过 $𝐩_t^{slc}=𝐩_t^{cmp}$ 获得选择块的重要性得分。对于分块方案不同的情况，我们根据它们的空间关系推导选择块的重要性得分。假设 $d∣l 且 d∣l′$，我们有：

![](../asserts/nsa-math9.png)

其中 [⋅] 表示索引操作符，用于访问向量元素。对于采用 GQA 或 MQA 架构的模型，其中键值缓存跨查询头共享，需要确保在这些头之间保持一致的块选择，以减少解码过程中 KV 缓存加载的次数。组内头之间共享的重要性得分正式定义为：

![](../asserts/nsa-math10.png)

其中 (h) 表示头索引，H 是每组中查询头的数量。这种聚合确保了在同一组内，头之间的块选择一致。

**Top-n 块选择**。在获得选择块的重要性得分后，我们保留通过块重要性得分排名前 n 的稀疏块，公式为：

![](../asserts/nsa-math11~12.png)

其中 rank(⋅) 表示按降序排列的位置，rank=1 对应最高得分，$ℐ_t$ 是选择块的索引集，Cat 表示拼接操作。$\tilde{K}_t^{slc}∈ℝ^{dk×n⁢l′}$ 是由压缩键组成的张量。对于细粒度值 $\tilde{V}_t^{slc}$ 也有类似的公式。然后，选中的键和值参与与查询 $𝐪_t$ 的注意力计算，定义如方程 5。

#### 3.3.3 滑动窗口

在注意力机制中，本地模式通常适应得更快，并且可能主导学习过程，这可能会阻止模型有效地从压缩和选择标记中学习。为了解决这个问题，我们引入了一个专门的滑动窗口分支，显式地处理本地上下文，允许其他分支（压缩和选择）专注于学习各自的特征，而不会被本地模式所简化。具体来说，我们在一个窗口 w 中维护最近的标记 $\tilde{K}_t^{win}=𝐤_{t−w}:t，\tilde{V}_t^{win}=𝐯_{t−w:t}$，并将来自不同信息源（压缩标记、选择标记、滑动窗口）的注意力计算隔离到不同的分支中。这些分支输出通过一个学习到的门控机制进行聚合。为了进一步防止不同注意力分支之间的快捷学习，同时减少计算开销，我们为三个分支提供独立的键和值。该架构设计通过防止局部模式与长程模式识别之间的梯度干扰，从而实现稳定学习，同时引入最小的开销。

在获得所有三类键和值（$\tilde{K}_t^{cmp}, \tilde{V}_t^{cmp}；\tilde{K}_t^{slc}, \tilde{V}_t^{slc}；\tilde{K}_t^{win}, \tilde{V}_t^{win}$）后，我们按照方程 5 计算最终的注意力输出。结合上述的压缩、选择和滑动窗口机制，这就形成了 NSA 完整的算法框架。

### 3.4 内核设计

为了在训练和预填充阶段实现 FlashAttention 级别的加速，我们在 Triton 上实现了与硬件对齐的稀疏注意力内核。由于多头自注意力（MHA）是内存密集型且对解码效率低下，我们专注于具有共享键值缓存的架构，如 GQA 和 MQA，这些架构遵循当前最先进的 LLM（大型语言模型）设计。尽管压缩和滑动窗口注意力计算与现有的 FlashAttention-2 内核兼容，但我们为稀疏选择注意力引入了专门的内核设计。如果我们遵循 FlashAttention 加载时序连续查询块到 SRAM 的策略，可能会导致低效的内存访问，因为块内的查询可能需要不连续的键值块。为了解决这个问题，我们的关键优化在于采用不同的查询分组策略：对于查询序列中的每个位置，我们将 GQA 组内的所有查询头（它们共享相同的稀疏键值块）加载到 SRAM 中。**图3** 展示了我们的前向传递实现。所提出的内核架构具有以下几个关键特征：

1. **基于组的数据加载**：对于每个内循环，在位置 t 加载组内所有头的查询 $Q∈ℝ^{[h, dk]}$ 及其共享的稀疏键/值块索引 $ℐ_t$。
2. **共享 KV 获取**：在内循环中，按顺序加载由 $ℐ_t$ 索引的连续键/值块到 SRAM 中，表示为 $K∈ℝ^{[Bk, dk]}, V∈ℝ^{[Bk, dv]}$，以最小化内存加载，其中 Bk 是满足 $Bk|l′$ 的内核块大小。
3. **网格外循环**：由于内循环的长度（与选中的块数 n 成正比）对于不同查询块几乎相同，因此我们将查询/输出循环放入 Triton 的网格调度器中，以简化和优化内核。

通过（1）通过组内共享消除冗余的 KV 传输，和（2）在 GPU 流多处理器之间平衡计算负载，这一设计实现了接近最优的算术强度。

![](../asserts/x3.png)

**图3：** NSA 的内核设计。内核通过 GQA 组（网格循环）加载查询，获取相应的稀疏 KV 块（内循环），并在 SRAM 上执行注意力计算。绿色块表示 SRAM 中的数据，蓝色表示 HBM 中的数据。